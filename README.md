ğŸ“œ Bhagavad Gita RAG System

Retrieval-Augmented Generation using LangChain, FAISS, and Groq

ğŸ“Œ Project Overview

This project implements a Retrieval-Augmented Generation (RAG) system that answers user queries strictly based on the Bhagavad Gita PDF.

Instead of relying on a general-purpose language model, the system first retrieves relevant content from the document and then generates answers grounded in that retrieved context. This avoids hallucinations and ensures document-faithful responses.

ğŸ¯ Objective

The objective of this project is to:

Build a document-grounded chatbot

Demonstrate a complete RAG pipeline

Use embeddings and vector search for retrieval

Integrate a Large Language Model (LLM) for answer generation

This project was implemented as part of the Shastra_AI RAG system task.

ğŸ§  What is Retrieval-Augmented Generation (RAG)?

RAG combines two core ideas:

Retrieval
Relevant document chunks are retrieved from a vector database using semantic similarity.

Generation
A language model generates an answer using both the user query and the retrieved context.

This ensures:

Answers come from the document

Reduced hallucinations

Explainable AI behavior

ğŸ—ï¸ System Architecture
Bhagavad Gita PDF
        â†“
Text Chunking
        â†“
Embeddings (Sentence Transformers)
        â†“
FAISS Vector Store
        â†“
Retriever
        â†“
Groq LLM
        â†“
Final Answer

ğŸ› ï¸ Technologies Used
Programming Language

Python 3.11

Libraries & Tools

LangChain â€“ RAG pipeline and chains

FAISS â€“ Vector similarity search

Sentence Transformers â€“ Text embeddings

Groq SDK â€“ Large Language Model inference

python-dotenv â€“ Environment variable management

PyPDF â€“ PDF loading

ğŸ“¦ Project Structure
shastra_rag_llm/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ bhagavad_gita.pdf
â”‚
â”œâ”€â”€ vectorstore/
â”‚   â”œâ”€â”€ index.faiss
â”‚   â””â”€â”€ index.pkl
â”‚
â”œâ”€â”€ ingest.py
â”œâ”€â”€ chatbot.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .env   (not included in submission)

âš™ï¸ How to Run This Project (Exact Steps)
ğŸ”¹ Prerequisites

Python 3.11.x

Internet connection

Groq API Key

Check Python version:

python --version

ğŸ”¹ Step 1: Create Virtual Environment
python -m venv venv


Activate it:

Windows

venv\Scripts\activate


You should see:

(venv)

ğŸ”¹ Step 2: Install Dependencies

All dependencies are listed in requirements.txt.

pip install -r requirements.txt


This installs LangChain, FAISS, Groq SDK, and all required libraries.

ğŸ”¹ Step 3: Set Up Environment Variables

Create a .env file in the project root directory:

GROQ_API_KEY=your_groq_api_key_here


âš ï¸ Do not share this file publicly.

ğŸ”¹ Step 4: Run Document Ingestion (One-Time)

This step processes the Bhagavad Gita PDF and creates the vector database.

python ingest.py


After successful execution, a vectorstore/ folder will be created.

ğŸ”¹ Step 5: Run the Chatbot

Start the RAG chatbot:

python chatbot.py


Expected output:

ğŸ“œ Bhagavad Gita RAG Chatbot Ready
Type 'exit' to quit

ğŸ”¹ Step 6: Ask Questions

Example questions:

What is Karma Yoga according to the Bhagavad Gita?

What advice does Krishna give to Arjuna?

Explain Nishkama Karma

What is Dharma in the Gita?

Type exit to stop the chatbot.

ğŸ§ª Validation (Proving It Is RAG)

Ask an unrelated question:

Who is the Prime Minister of India?


The system should not answer correctly, proving that responses are limited to the provided document.

ğŸ“š Concepts Used in This Project

Retrieval-Augmented Generation (RAG)

Vector embeddings

Cosine similarity

FAISS indexing

Prompt grounding

LLM API integration

Environment variable security

ğŸ§¹ Notes for Submission

venv/ folder is not included

.env file is not shared

Dependencies are reproducible using requirements.txt

ğŸš€ Future Enhancements

Web interface (Streamlit / React)

Support for multiple PDFs

Answer citations

Cloud deployment

User authentication

ğŸ§¾ Conclusion

This project demonstrates a complete and practical implementation of a Retrieval-Augmented Generation system. It shows how modern LLMs can be safely combined with document retrieval to produce accurate, grounded, and explainable responses.